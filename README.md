# Deepfake Detection via Remote Photoplethysmography (rPPG)

This project is a comprehensive implementation and evaluation of a deepfake detection methodology based on the 2025 research paper, *"Towards Robust Deepfake Detection Based on Heart Rate Analysis"* by Soo-Hyun Lee et al.

The primary goal is to extract physiological signals (heart rate variability) from video footage to distinguish between authentic and deepfake videos. This repository contains a full pipeline to:
1.  Extract rPPG signals from videos using advanced, motion-robust algorithms.
2.  Perform feature engineering to quantify signal variability.
3.  Generate a complete, model-ready dataset.
4.  Train and evaluate a machine learning classifier for deepfake detection.

## Project Structure

```
.
├── create_dataset.py       # Main script to process videos and generate the full feature dataset.
├── train.py                # Script to train and evaluate the LightGBM detection model.
├── model/
│   └── deepfake_detector_model.joblib # The final, saved, trained model.
├── output/
│   ├── dataset/
│   │   ├── fake.csv        # Generated features (30 per video) for fake videos.
│   │   └── real.csv        # Generated features (30 per video) for real videos.
│   └── plots/                # (Generated by create_dataset.py, if configured)
│       ├── real/
│       └── fake/
├── analysis.py             # (Legacy) Earlier version of the data creation script.
├── copy_files.py           # (Utility) Script for managing video data subsets.
├── main.py                 # (Legacy) Initial single-video analysis script.
├── pyproject.toml          # Python project configuration for uv.
├── README.md               # This file.
└── uv.lock                 # Lockfile for reproducible dependencies.
```
*(Note: `output/scores` and `output/dataset_subset` appear to be artifacts from previous experimental runs.)*

## Methodology

This implementation follows a rigorous, multi-stage pipeline designed to replicate and test the findings of the source research paper. After a series of iterative refinements to improve signal quality, the final, most robust methodology is as follows:

### 1. Signal Extraction (rPPG)

The core of the project is extracting a clean Blood Volume Pulse (BVP) signal. The final implementation uses the **CHROM (Chrominance-based)** method for signal extraction due to its superior robustness against head motion and lighting fluctuations, which were major sources of noise in simpler methods.

### 2. Heart Rate and Variability Calculation

From the clean CHROM signal, a time-series of heart rate (HR) values is derived using a multi-step process:
1.  **Bandpass Filtering:** The signal is filtered to keep only frequencies corresponding to a plausible human heart rate (40-240 BPM).
2.  **Signal Smoothing:** A **Savitzky-Golay filter** is applied to remove high-frequency noise and clarify the underlying pulse shape.
3.  **Peak Detection:** Individual heartbeats (peaks) are identified in the smoothed signal.
4.  **HR Conversion:** The time intervals between peaks are converted into a list of instantaneous HR values.

### 3. Feature Engineering

Based on the HR time-series, three distinct features inspired by medical Heart Rate Variability (HRV) analysis are calculated:

*   **`feat#1` (SDNN-inspired):** Measures *overall* signal variability.
*   **`feat#2` (RMSSD-inspired):** Measures short-term *jitter*.
*   **`feat#3` (SDSD-inspired):** Measures the *consistency* of the jitter.

This process results in a comprehensive **30-feature vector** (3 features for each of the 9 facial regions and the average signal) for each video.

### 4. Model Training

The generated feature set is used to train a **LightGBM (Light Gradient Boosting Machine)** classifier, a powerful model for finding patterns in tabular data.

## How to Use This Project

### Step 1: Installation

This project uses `uv` for dependency management. Install the required libraries:

```bash
pip install uv
uv pip install -r requirements.txt # (Assuming you have a requirements.txt, otherwise install from pyproject.toml)
# Or manually:
pip install opencv-python mediapipe numpy matplotlib scipy pandas tqdm lightgbm scikit-learn seaborn joblib
```

### Step 2: Prepare Your Video Data

Place your real videos and fake videos into separate directories.

### Step 3: Generate the Training Dataset

1.  Open `create_dataset.py`.
2.  Update the `real_video_directories` and `fake_video_directories` variables to point to your data folders.
3.  Run the script from your terminal:
    ```bash
    python create_dataset.py
    ```
This will process all videos and generate `real.csv` and `fake.csv` in `output/dataset/`.

### Step 4: Train the Detector Model

1.  Open `train.py`.
2.  Ensure the paths to the CSV files in `output/dataset/` are correct.
3.  Run the script from your terminal:
    ```bash
    python train.py
    ```
This script will train the model, evaluate its performance, display a confusion matrix, and save the final trained model to `model/deepfake_detector_model.joblib`.

## Experimental Results and Conclusion

This project successfully implemented a robust pipeline for rPPG-based deepfake detection. However, after systematic experimentation with a custom dataset of 200 videos (100 real, 100 fake), the final conclusion was that **this methodology was not effective for this specific dataset.**

### Key Findings:

*   **Iterative Refinement:** The methodology was progressively improved, moving from simple (GREEN channel) to advanced (CHROM) signal extraction and from noisy (FFT) to robust (smoothed peak detection) HR calculation.
*   **Persistent Learning Failure:** Despite these significant technical improvements, the final LightGBM model performed at **35% accuracy**, which is worse than random chance.
*   **Feature Insignificance:** The model's internal diagnostics consistently reported `No further splits with positive gain`, indicating that the 30 extracted features were not statistically different enough between the real and fake video sets for the model to learn a predictive pattern.

### Final Conclusion

The experiment demonstrates that the viability of rPPG as a deepfake detection method is **highly dependent on the nature of the video dataset**. Factors such as **video compression, the specific deepfake generation algorithm used, and subject conditions (lighting, motion)** can either completely erase the subtle rPPG signal or create fake videos where the signal is not significantly different from a real one.

While this project did not result in a high-accuracy detector, it serves as a valuable and conclusive experiment, demonstrating the practical limitations of this specific physiological approach on certain types of video data.